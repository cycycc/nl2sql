{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0596ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9812e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wikisql (/Users/alexwang/.cache/huggingface/datasets/wikisql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6907b253314083bc0e869ada5a9792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "all_data = datasets.load_dataset('wikisql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac5bca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'validation', 'train'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data['train'][0]['table']\n",
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0527851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: model must be cased\n",
    "# NB: position of question vs table\n",
    "\n",
    "def format_input(example, input_token=\"<i>\", col_tok=\"<c>\", typ_tok=\"<t>\"):\n",
    "    # input_token: string for separating table from question\n",
    "    question = example['question']\n",
    "    \n",
    "    table = example['table']\n",
    "    table_name = table['name'] if table['name'] else table['id']\n",
    "    cols = table['header']\n",
    "    types = table['types']\n",
    "    linearized_table = f'{table_name} '\n",
    "    for col, typ in zip(cols, types):\n",
    "        linearized_table += f'{col_tok} {col} {typ_tok} {typ} '\n",
    "    \n",
    "    return f\"{linearized_table} {input_token} {question}\"\n",
    "    \n",
    "    \n",
    "def format_output(example):\n",
    "    target = example['sql']['human_readable']\n",
    "    \n",
    "    table = example['table']\n",
    "    table_name = table['name'] if table['name'] else table['id']\n",
    "    target = target.replace('FROM table', f'FROM {table_name}')\n",
    "    return target\n",
    "    \n",
    "    \n",
    "def format_example(example, sep_token=\"<s>\", input_token=\"<i>\",\n",
    "                   col_tok=\"<c>\", typ_tok=\"<t>\"):\n",
    "    example_input = format_input(example, input_token=input_token)\n",
    "    example_output = format_output(example)\n",
    "    return f\"{example_input} {sep_token} {example_output}\"\n",
    "\n",
    "\n",
    "def process_dataset(data, max_examples=-1,\n",
    "                    example_separator=\"--SEPARATOR--\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_exs = 0\n",
    "    processed_examples = []\n",
    "    for example in data:\n",
    "        processed_examples.append(format_example(example))\n",
    "        n_exs += 1\n",
    "        if max_examples > 0 and n_exs >= max_examples:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\tProcessed {len(processed_examples)} examples\")\n",
    "    return f\"\\n{example_separator}\\n\".join(processed_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd95960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train\n",
      "\tProcessed 10000 examples\n",
      "Processing validation\n",
      "\tProcessed 8421 examples\n",
      "Processing test\n",
      "\tProcessed 10000 examples\n"
     ]
    }
   ],
   "source": [
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "max_examples=10000\n",
    "\n",
    "for split in SPLITS:\n",
    "    split_data = all_data[split]\n",
    "    print(f\"Processing {split}\")\n",
    "    processed_data = process_dataset(split_data, max_examples=max_examples)\n",
    "    with open(f'data/{split}.nexs{max_examples}.txt', 'w') as out_fh:\n",
    "        out_fh.write(processed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
