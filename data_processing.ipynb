{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0596ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import ipdb\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ac1b7",
   "metadata": {},
   "source": [
    "# WikiSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9812e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wikisql (/Users/alexwang/.cache/huggingface/datasets/wikisql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033395c1235545588f95906ab403a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "all_data = datasets.load_dataset('wikisql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "667cbe3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_readable': 'SELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory',\n",
       " 'sel': 1,\n",
       " 'agg': 0,\n",
       " 'conds': {'column_index': [0],\n",
       "  'operator_index': [0],\n",
       "  'condition': ['Australian Capital Territory']}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['train'][3]['sql']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e85a17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    ' ': '-',\n",
    "    '/': '-',\n",
    "    '.': '',\n",
    "    '#': 'Number',\n",
    "}\n",
    "\n",
    "\n",
    "def process_column_name(col):\n",
    "    \n",
    "    for k, v in to_replace.items():\n",
    "        col = col.replace(k, v)\n",
    "    return col\n",
    "    \n",
    "\n",
    "def process_tables(examples):\n",
    "    \"\"\" For each table, format them to be BigQuery ready:\n",
    "        * remove whitespace from table names\n",
    "        * quote strings\n",
    "    \"\"\"\n",
    "    \n",
    "    new_examples = []\n",
    "    for example in examples:\n",
    "        table = example['table']\n",
    "\n",
    "        sql = example['sql']\n",
    "        target = sql['human_readable']\n",
    "        for condition in sql['conds']['condition']:\n",
    "            target = target.replace(condition, f'\\\"{condition}\\\"')\n",
    "        \n",
    "        cols = table['header']\n",
    "        new_cols = []\n",
    "        \n",
    "        for col in cols:\n",
    "            new_col = process_column_name(col)\n",
    "            new_cols.append(new_col)\n",
    "            target = target.replace(col, new_col)\n",
    "            \n",
    "            \n",
    "        example['modified_sql'] = target\n",
    "        table['modified_header'] = new_cols\n",
    "        example['table'] = table\n",
    "        new_examples.append(example)\n",
    "        \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e15c7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "#SPLITS = [\"validation\"]\n",
    "\n",
    "split2data = {}\n",
    "for split in SPLITS:\n",
    "    split2data[split] = process_tables(all_data[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5bca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': 1,\n",
       " 'question': 'What position does the player who played for butler cc (ks) play?',\n",
       " 'table': {'header': ['Player',\n",
       "   'No',\n",
       "   'Nationality',\n",
       "   'Position',\n",
       "   'Years-in-Toronto',\n",
       "   'School-Club-Team'],\n",
       "  'page_title': 'Toronto Raptors all-time roster',\n",
       "  'page_id': '',\n",
       "  'types': ['text', 'text', 'text', 'text', 'text', 'text'],\n",
       "  'id': '1-10015132-11',\n",
       "  'section_title': 'L',\n",
       "  'caption': 'L',\n",
       "  'rows': [['Antonio Lang',\n",
       "    '21',\n",
       "    'United States',\n",
       "    'Guard-Forward',\n",
       "    '1999-2000',\n",
       "    'Duke'],\n",
       "   ['Voshon Lenard', '2', 'United States', 'Guard', '2002-03', 'Minnesota'],\n",
       "   ['Martin Lewis',\n",
       "    '32, 44',\n",
       "    'United States',\n",
       "    'Guard-Forward',\n",
       "    '1996-97',\n",
       "    'Butler CC (KS)'],\n",
       "   ['Brad Lohaus', '33', 'United States', 'Forward-Center', '1996', 'Iowa'],\n",
       "   ['Art Long',\n",
       "    '42',\n",
       "    'United States',\n",
       "    'Forward-Center',\n",
       "    '2002-03',\n",
       "    'Cincinnati'],\n",
       "   ['John Long', '25', 'United States', 'Guard', '1996-97', 'Detroit'],\n",
       "   ['Kyle Lowry', '3', 'United States', 'Guard', '2012-Present', 'Villanova']],\n",
       "  'name': 'table_10015132_11'},\n",
       " 'sql': {'human_readable': 'SELECT Position FROM table WHERE School/Club Team = Butler CC (KS)',\n",
       "  'sel': 3,\n",
       "  'agg': 0,\n",
       "  'conds': {'column_index': [5],\n",
       "   'operator_index': [0],\n",
       "   'condition': ['Butler CC (KS)']}},\n",
       " 'modified_sql': 'SELECT Position FROM table WHERE School-Club-Team = \"Butler CC (KS)\"'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data['train'][0]['table']\n",
    "split2data['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa2b50",
   "metadata": {},
   "source": [
    "### Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e0527851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: model must be cased\n",
    "# NB: position of question vs table\n",
    "\n",
    "def format_input(example, use_modified_fields=True,\n",
    "                 input_token=\"<i>\",\n",
    "                 col_tok=\"<c>\", typ_tok=\"<t>\",\n",
    "                 use_prefix=False, prefix=\"Generate SQL for the question:\",\n",
    "                 augmentation_factor=1):\n",
    "    # input_token: string for separating table from question\n",
    "    question = example['question']\n",
    "    \n",
    "    table = example['table']\n",
    "    table_name = table['name'] if table['name'] else table['id']\n",
    "    cols = table['modified_header'] if use_modified_fields else table['header']\n",
    "    types = table['types']\n",
    "    cols_and_types = list(zip(cols, types))\n",
    "    \n",
    "    assert isinstance(augmentation_factor, int), \"Augmentation factor must be an integer!\"\n",
    "    linearized_tables = []\n",
    "\n",
    "    for aug_idx in range(augmentation_factor):  \n",
    "        \n",
    "        linearized_table = f'{table_name}'\n",
    "        \n",
    "        if aug_idx > 0: # zero-index\n",
    "            # keep table columns in order to use original data\n",
    "            random.shuffle(cols_and_types)\n",
    "        \n",
    "        # shuffle (col and typ)\n",
    "        for col, typ in cols_and_types:\n",
    "            linearized_table += f' {col_tok} {col} {typ_tok} {typ}'\n",
    "    \n",
    "        if use_prefix:\n",
    "            input_str = f\"{prefix} {linearized_table} {input_token} {question}\"\n",
    "        else:\n",
    "            input_str = f\"{linearized_table} {input_token} {question}\"\n",
    "            \n",
    "        linearized_tables.append(f\"{linearized_table} {input_token} {question}\".strip())\n",
    "    \n",
    "    return linearized_tables\n",
    "    \n",
    "    \n",
    "def format_output(example, use_modified_fields=True):\n",
    "    target = example['modified_sql'] if use_modified_fields else example['sql']['human_readable']\n",
    "    \n",
    "    table = example['table']\n",
    "    table_name = table['name'] if table['name'] else table['id']\n",
    "    target = target.replace('FROM table', f'FROM {table_name}')\n",
    "    return target\n",
    "    \n",
    "    \n",
    "def format_example(example, use_modified_fields=True,\n",
    "                   sep_token=\"<s>\", end_tok=\"</s>\",\n",
    "                   input_token=\"<i>\",\n",
    "                   col_tok=\"<c>\", typ_tok=\"<t>\",\n",
    "                   augmentation_factor=1):\n",
    "    example_inputs = format_input(example, input_token=input_token, \n",
    "                                  use_modified_fields=use_modified_fields,\n",
    "                                  augmentation_factor=augmentation_factor)\n",
    "    example_output = format_output(example, use_modified_fields=use_modified_fields)\n",
    "    examples = [f\"{example_input} {sep_token} {example_output} {end_tok}\" for example_input in example_inputs]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def process_dataset(data, max_examples=-1,\n",
    "                    example_separator=\"--SEPARATOR--\",\n",
    "                    use_modified_fields=True,\n",
    "                    augmentation_factor=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_exs = 0\n",
    "    processed_examples = []\n",
    "    for example in data:\n",
    "        processed_examples += format_example(example, use_modified_fields=use_modified_fields, \n",
    "                                             augmentation_factor=augmentation_factor)\n",
    "        n_exs += 1\n",
    "        if max_examples > 0 and n_exs >= max_examples:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\tProcessed {len(processed_examples)} examples\")\n",
    "    return f\"\\n{example_separator}\\n\".join(processed_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af9219d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train\n",
      "\tProcessed 281775 examples\n",
      "\tWrote to data/wikisql/bigquery/train.aug5.txt\n",
      "Processing validation\n",
      "\tProcessed 42105 examples\n",
      "\tWrote to data/wikisql/bigquery/validation.aug5.txt\n",
      "Processing test\n",
      "\tProcessed 79390 examples\n",
      "\tWrote to data/wikisql/bigquery/test.aug5.txt\n"
     ]
    }
   ],
   "source": [
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "#SPLITS = [\"validation\"]\n",
    "max_examples = -1\n",
    "use_prefix = False\n",
    "use_modified_fields = True\n",
    "augmentation_factor = 5\n",
    "\n",
    "for split in SPLITS:\n",
    "    #split_data = all_data[split]\n",
    "    split_data = split2data[split]\n",
    "    print(f\"Processing {split}\")\n",
    "    processed_data = process_dataset(split_data, max_examples=max_examples,\n",
    "                                     use_modified_fields=use_modified_fields,\n",
    "                                     augmentation_factor=augmentation_factor)\n",
    "    \n",
    "    split_file_name = f'{split}'\n",
    "    if max_examples > 0:\n",
    "        split_file_name += f'.nexs{max_examples}'\n",
    "    if use_prefix:\n",
    "        split_file_name += '.prefix'\n",
    "    if augmentation_factor > 1:\n",
    "        split_file_name += f'.aug{augmentation_factor}'\n",
    "    out_dir = 'bigquery' if use_modified_fields else 'wikisql'\n",
    "    split_file_name = f'data/wikisql/{out_dir}/{split_file_name}.txt'\n",
    "    \n",
    "    with open(split_file_name, 'w') as out_fh:\n",
    "        out_fh.write(processed_data)\n",
    "    print(f'\\tWrote to {split_file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c8362",
   "metadata": {},
   "source": [
    "### Extract Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a4c4fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tables from train\n",
      "Extracting tables from validation\n",
      "Extracting tables from test\n"
     ]
    }
   ],
   "source": [
    "def extract_tables(data, do_write=True):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    \n",
    "    all_tables = {}\n",
    "    \n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        print(f\"Extracting tables from {split}\")\n",
    "        for example in data[split]:\n",
    "            table = example[\"table\"]\n",
    "            table_name = table['name'] if table['name'] else table['id']\n",
    "            \n",
    "            if table_name in all_tables:\n",
    "                assert all_tables[table_name] == table, \"Table name collision\"\n",
    "            else:\n",
    "                all_tables[table_name] = table\n",
    "            \n",
    "    all_tables = list(all_tables.values())\n",
    "    if do_write:\n",
    "        with open(\"data/all_tables.jsonl\", 'w', encoding='utf-8') as out_fh:\n",
    "            for table in all_tables:\n",
    "                out_fh.write(f'{json.dumps(table)}\\n')\n",
    "\n",
    "        print(f\"Wrote {len(all_tables)} tables\")\n",
    "        \n",
    "    return all_tables\n",
    "    \n",
    "all_tables = extract_tables(split2data, do_write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7feb7182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_colss = np.array([len(t['header']) for t in all_tables])\n",
    "n_colss.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d9985b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading \n",
    "tables = [json.loads(l) for l in open(\"data/all_tables.jsonl\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96676805",
   "metadata": {},
   "source": [
    "# Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c55d271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset spider (/Users/alexwang/.cache/huggingface/datasets/spider/spider/1.0.0/4e5143d825a3895451569c8b9b55432b91a4bc2d04d390376c950837f4680daa)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631ab180b2d94a5fbbd4e46630415800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_spider = datasets.load_dataset('spider')\n",
    "spider_tables = {t['db_id']: t for t in json.load(open('data/spider/tables.json', encoding='utf-8'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36a5f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_spider_table(table,                    \n",
    "                           sep_token=\"<s>\", end_tok=\"</s>\",\n",
    "                           input_token=\"<i>\",\n",
    "                           col_tok=\"<c>\", typ_tok=\"<t>\",\n",
    "                           augmentation_factor=1):\n",
    "    \"\"\" Expects tables as a dict (see below function) \n",
    "    \"\"\"\n",
    "\n",
    "    table_name = table['name']\n",
    "    cols = table['columns']\n",
    "    types = table['types']\n",
    "\n",
    "    linearized_table = f'{table_name}'\n",
    "    for col, typ in zip(cols, types):\n",
    "        linearized_table += f' {col_tok} {col} {typ_tok} {typ}'\n",
    "\n",
    "    return linearized_table\n",
    "\n",
    "\n",
    "def linearize_spider_tables(tables, table_separator=\"</t>\",\n",
    "                            augmentation_factor=1):\n",
    "    id2db = defaultdict(list)\n",
    "    \n",
    "    for db_name, db in spider_tables.items():\n",
    "        \n",
    "        all_table_names = db['table_names_original']\n",
    "        idx2table = {k: {'name': v, 'columns': [], 'types': []} for k, v in enumerate(all_table_names)}\n",
    "\n",
    "        # map columns to tables\n",
    "        all_cols = db['column_names_original']\n",
    "        all_types = db['column_types']\n",
    "        for col, typ in zip(all_cols, all_types):\n",
    "            table_idx, col_name = col\n",
    "            if table_idx not in idx2table:\n",
    "                continue\n",
    "            idx2table[table_idx]['columns'].append(col_name)\n",
    "            idx2table[table_idx]['types'].append(typ)\n",
    "            \n",
    "        # linearize tables and join them into one long string\n",
    "        linearized_db = []\n",
    "        for table in list(idx2table.values()):\n",
    "            linearized_db.append(linearize_spider_table(table))\n",
    "            \n",
    "        for aug_idx in range(augmentation_factor):\n",
    "            if augmentation_factor > 0:\n",
    "                random.shuffle(linearized_db)\n",
    "\n",
    "            id2db[db_name].append(f\" {table_separator} \".join(linearized_db))\n",
    "        \n",
    "    return id2db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "13fa4414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people <c> People_ID <t> number <c> Name <t> text <c> Height <t> number <c> Weight <t> number <c> Home Town <t> text </t> perpetrator <c> Perpetrator_ID <t> number <c> People_ID <t> number <c> Date <t> text <c> Year <t> number <c> Location <t> text <c> Country <t> text <c> Killed <t> number <c> Injured <t> number',\n",
       " 'perpetrator <c> Perpetrator_ID <t> number <c> People_ID <t> number <c> Date <t> text <c> Year <t> number <c> Location <t> text <c> Country <t> text <c> Killed <t> number <c> Injured <t> number </t> people <c> People_ID <t> number <c> Name <t> text <c> Height <t> number <c> Weight <t> number <c> Home Town <t> text']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2db['perpetrator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4ace5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: model must be cased\n",
    "# NB: position of question vs table\n",
    "\n",
    "def format_spider_input(example,\n",
    "                        input_token=\"<i>\",\n",
    "                        use_prefix=False, prefix=\"Generate SQL for the question:\",\n",
    "                        augmentation_factor=1):\n",
    "    # input_token: string for separating table from question\n",
    "    question = example['question']\n",
    "    \n",
    "    table_id = example['db_id']\n",
    "    linearized_dbs = id2db[table_id]\n",
    "    linearized_inputs = []\n",
    "    for linearized_db in linearized_dbs:\n",
    "        \n",
    "        if use_prefix:\n",
    "            # TODO(AW): add prefix separator?\n",
    "            input_str = f\"{prefix} {linearized_db} {input_token} {question}\"\n",
    "        else:\n",
    "            input_str = f\"{linearized_db} {input_token} {question}\"\n",
    "    \n",
    "        linearized_inputs.append(f\"{input_str}\".strip())\n",
    "        \n",
    "    return linearized_inputs\n",
    "    \n",
    "    \n",
    "def format_spider_output(example):\n",
    "    target = example['query']\n",
    "    return target\n",
    "    \n",
    "    \n",
    "def format_spider(example,\n",
    "                   sep_token=\"<s>\", end_tok=\"</s>\",\n",
    "                   input_token=\"<i>\",\n",
    "                   col_tok=\"<c>\", typ_tok=\"<t>\",\n",
    "                   augmentation_factor=1):\n",
    "    example_inputs = format_spider_input(example, input_token=input_token,\n",
    "                                         augmentation_factor=augmentation_factor)\n",
    "    example_output = format_spider_output(example)\n",
    "    examples = [f\"{example_input} {sep_token} {example_output} {end_tok}\" for example_input in example_inputs]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def process_spider_split(data, max_examples=-1,\n",
    "                         example_separator=\"--SEPARATOR--\",\n",
    "                         augmentation_factor=1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_exs = 0\n",
    "    processed_examples = []\n",
    "    for example in data:\n",
    "        processed_examples += format_spider(example, augmentation_factor=augmentation_factor)\n",
    "        n_exs += 1\n",
    "        if max_examples > 0 and n_exs >= max_examples:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\tProcessed {len(processed_examples)} examples\")\n",
    "    return f\"\\n{example_separator}\\n\".join(processed_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e1ec9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train\n",
      "\tProcessed 14000 examples\n",
      "\tWrote to data/spider/train.aug2.txt\n",
      "Processing validation\n",
      "\tProcessed 2068 examples\n",
      "\tWrote to data/spider/validation.aug2.txt\n"
     ]
    }
   ],
   "source": [
    "SPLITS = [\"train\", \"validation\"]\n",
    "#SPLITS = [\"validation\"]\n",
    "max_examples = -1\n",
    "use_prefix = False\n",
    "use_modified_fields = False\n",
    "\n",
    "augmentation_factor = 2\n",
    "id2db = linearize_spider_tables(spider_tables, augmentation_factor=augmentation_factor)\n",
    "\n",
    "for split in SPLITS:\n",
    "    #split_data = all_data[split]\n",
    "    split_data = all_spider[split]\n",
    "    print(f\"Processing {split}\")\n",
    "    processed_data = process_spider_split(split_data, max_examples=max_examples)\n",
    "    \n",
    "    split_file_name = f'{split}'\n",
    "    if max_examples > 0:\n",
    "        split_file_name += f'.nexs{max_examples}'\n",
    "    if use_prefix:\n",
    "        split_file_name += '.prefix'\n",
    "    if augmentation_factor > 1:\n",
    "        split_file_name += f'.aug{augmentation_factor}'\n",
    "    #out_dir = 'bigquery' if use_modified_fields else 'wikisql'\n",
    "    split_file_name = f'data/spider/{split_file_name}.txt'\n",
    "    \n",
    "    with open(split_file_name, 'w') as out_fh:\n",
    "        out_fh.write(processed_data)\n",
    "    print(f'\\tWrote to {split_file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
